# BIG-DATA-ANALYSIS

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: PABITRA SAHOO

*INTERN ID*: CT04DF1804

*DOMAIN*: DATA ANALYST

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTOSH

## TASK DESCRIPTION

🔍 Project Overview
This project demonstrates the ability to perform big data analysis at scale using Dask, a parallel computing library in Python. The goal was to process a large dataset efficiently and derive meaningful insights using distributed computation.

⚙️ Tools & Technologies
Language: Python
Library: Dask (for parallel data processing)
Visualization: Matplotlib / Plotly (if included)
Dataset: Simulated large dataset processed in chunks

📊 Dataset Description
The dataset contains three main columns:
C: Categorical variable (e.g., group/category)
A: Continuous numerical value (e.g., average metric)
B: Count or aggregated metric (e.g., frequency or volume)

🧪 Task Performed
Loaded a large dataset using Dask DataFrame
Performed group-by operations on column C
Aggregated metrics:
Mean of column A
Sum or count of column B
## Output was a concise summary table with grouped insights
![Dashboard](Analysis.png)

📈 Insights
Category Z has the highest average in metric A (0.5492)
Category Z also has the highest volume in B (1747), indicating strong presence and performance
Category X shows the lowest average and volume, possibly a low-performing or underrepresented group

📌 Conclusion
This project successfully showcases how Dask can be used for handling large-scale data analysis tasks efficiently. It highlights the benefits of using Dask over traditional pandas when working with big data.



